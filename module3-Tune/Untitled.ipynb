{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5cfeb6-ebd7-4b60-b11e-fe187235f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b9076e5-0815-4d31-9a6b-397fa0f54306",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64da398b-f7aa-420f-8011-419f07d17ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train.shape)\n",
    "display(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee03a73b-1227-4002-838f-6e13f0df420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale pixel values between 0 and 1\n",
    "max_pixel_value = 255\n",
    "X_train = X_train / max_pixel_value\n",
    "X_test = X_test / max_pixel_value\n",
    "\n",
    "# flatten images into row vectors\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18ef7b73-1ae9-4953-bc36-f3d96153cf23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.61361015e-05, 2.76816609e-04, 2.76816609e-04, 2.76816609e-04,\n",
       "       1.93771626e-03, 2.09150327e-03, 2.69127259e-03, 3.99846213e-04,\n",
       "       2.55286428e-03, 3.92156863e-03, 3.79853902e-03, 1.95309496e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       4.61361015e-04, 5.53633218e-04, 1.44559785e-03, 2.36831988e-03,\n",
       "       2.61437908e-03, 3.89081123e-03, 3.89081123e-03, 3.89081123e-03,\n",
       "       3.89081123e-03, 3.89081123e-03, 3.46020761e-03, 2.64513649e-03,\n",
       "       3.89081123e-03, 3.72164552e-03, 2.99884660e-03, 9.84236832e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 7.53556324e-04,\n",
       "       3.66013072e-03, 3.89081123e-03, 3.89081123e-03, 3.89081123e-03,\n",
       "       3.89081123e-03, 3.89081123e-03, 3.89081123e-03, 3.89081123e-03,\n",
       "       3.89081123e-03, 3.86005383e-03, 1.43021915e-03, 1.26105344e-03,\n",
       "       1.26105344e-03, 8.61207228e-04, 5.99769319e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.76816609e-04,\n",
       "       3.36793541e-03, 3.89081123e-03, 3.89081123e-03, 3.89081123e-03,\n",
       "       3.89081123e-03, 3.89081123e-03, 3.04498270e-03, 2.79892349e-03,\n",
       "       3.79853902e-03, 3.70626682e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.23029604e-03, 2.39907728e-03, 1.64552095e-03, 3.89081123e-03,\n",
       "       3.89081123e-03, 3.15263360e-03, 1.69165705e-04, 0.00000000e+00,\n",
       "       6.61284121e-04, 2.36831988e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.15301807e-04, 1.53787005e-05, 2.36831988e-03,\n",
       "       3.89081123e-03, 1.38408304e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.13763937e-03,\n",
       "       3.89081123e-03, 2.92195309e-03, 3.07574010e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.69165705e-04,\n",
       "       2.92195309e-03, 3.89081123e-03, 1.07650903e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       5.38254517e-04, 3.70626682e-03, 3.46020761e-03, 2.46059208e-03,\n",
       "       1.66089965e-03, 1.53787005e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 1.24567474e-03, 3.69088812e-03, 3.89081123e-03,\n",
       "       3.89081123e-03, 1.83006536e-03, 3.84467512e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 6.92041522e-04, 2.86043829e-03,\n",
       "       3.89081123e-03, 3.89081123e-03, 2.30680507e-03, 4.15224913e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.46059208e-04,\n",
       "       1.43021915e-03, 3.87543253e-03, 3.89081123e-03, 2.87581699e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 3.82929642e-03, 3.89081123e-03, 3.82929642e-03,\n",
       "       9.84236832e-04, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 7.07420223e-04, 1.99923106e-03,\n",
       "       2.81430219e-03, 3.89081123e-03, 3.89081123e-03, 3.18339100e-03,\n",
       "       3.07574010e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       5.99769319e-04, 2.27604767e-03, 3.52172241e-03, 3.89081123e-03,\n",
       "       3.89081123e-03, 3.89081123e-03, 3.84467512e-03, 2.79892349e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.69088812e-04, 1.75317186e-03,\n",
       "       3.39869281e-03, 3.89081123e-03, 3.89081123e-03, 3.89081123e-03,\n",
       "       3.89081123e-03, 3.09111880e-03, 1.19953864e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.53710111e-04, 1.01499423e-03, 3.27566321e-03, 3.89081123e-03,\n",
       "       3.89081123e-03, 3.89081123e-03, 3.89081123e-03, 3.04498270e-03,\n",
       "       1.24567474e-03, 3.07574010e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 2.76816609e-04, 2.62975779e-03,\n",
       "       3.36793541e-03, 3.89081123e-03, 3.89081123e-03, 3.89081123e-03,\n",
       "       3.89081123e-03, 2.99884660e-03, 1.23029604e-03, 1.38408304e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       8.45828527e-04, 2.64513649e-03, 3.47558631e-03, 3.89081123e-03,\n",
       "       3.89081123e-03, 3.89081123e-03, 3.89081123e-03, 3.75240292e-03,\n",
       "       2.04536717e-03, 1.69165705e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       2.09150327e-03, 3.89081123e-03, 3.89081123e-03, 3.89081123e-03,\n",
       "       3.26028451e-03, 2.07612457e-03, 2.02998847e-03, 2.46059208e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aefce8f-d742-46a7-bd65-d05fc464f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "055bf596-7aca-4b96-8937-0066cdfcc1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1r/07vgg77n2vd0vy3wf99ryrmh0000gn/T/ipykernel_17698/1595574228.py:19: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
      "  model = KerasClassifier(build_fn=create_model, verbose=1)\n",
      "2021-12-06 17:45:10.425981: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.430017: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.459330: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.459613: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.461093: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.463156: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.465505: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.465662: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.468935: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.469178: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.471320: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.477768: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.478072: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.489915: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.492575: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-06 17:45:10.516159: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   1/6000 [..............................] - ETA: 1:03:24 - loss: 2.3028 - accuracy: 0.1250Epoch 1/3\n",
      "   1/6000 [..............................] - ETA: 1:03:41 - loss: 2.3026 - accuracy: 0.1250Epoch 1/5\n",
      "   1/6000 [..............................] - ETA: 1:03:29 - loss: 2.3022 - accuracy: 0.2500Epoch 1/5\n",
      "   1/6000 [..............................] - ETA: 1:04:04 - loss: 2.3024 - accuracy: 0.0000e+00Epoch 1/3\n",
      "  25/6000 [..............................] - ETA: 12s - loss: 2.3016 - accuracy: 0.1250    Epoch 1/3\n",
      "   1/1500 [..............................] - ETA: 16:21 - loss: 2.3023 - accuracy: 0.1875Epoch 1/3\n",
      "   1/6000 [..............................] - ETA: 1:05:13 - loss: 2.3025 - accuracy: 0.2500Epoch 1/3\n",
      "   1/1500 [..............................] - ETA: 16:08 - loss: 2.3023 - accuracy: 0.0625Epoch 1/3\n",
      "   1/6000 [..............................] - ETA: 1:05:35 - loss: 2.3031 - accuracy: 0.0000e+00Epoch 1/3\n",
      "   1/6000 [..............................] - ETA: 1:05:24 - loss: 2.3021 - accuracy: 0.1250Epoch 1/5\n",
      "  10/6000 [..............................] - ETA: 34s - loss: 2.3036 - accuracy: 0.0875    Epoch 1/3\n",
      "  15/1500 [..............................] - ETA: 5s - loss: 2.3014 - accuracy: 0.1562   Epoch 1/5\n",
      "   9/1500 [..............................] - ETA: 9s - loss: 2.3018 - accuracy: 0.0972   Epoch 1/3\n",
      "  48/6000 [..............................] - ETA: 19s - loss: 2.3000 - accuracy: 0.1406Epoch 1/3\n",
      "  38/1500 [..............................] - ETA: 6s - loss: 2.2991 - accuracy: 0.1661Epoch 1/5\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 1.6919 - accuracy: 0.5723\n",
      "1469/1500 [============================>.] - ETA: 0s - loss: 1.7185 - accuracy: 0.5801Epoch 2/3\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 1.7060 - accuracy: 0.5832\n",
      "1514/6000 [======>.......................] - ETA: 19s - loss: 2.0690 - accuracy: 0.3931Epoch 2/3\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 1.7476 - accuracy: 0.5702\n",
      "Epoch 2/3\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 1.6850 - accuracy: 0.5905\n",
      "1539/6000 [======>.......................] - ETA: 19s - loss: 2.0634 - accuracy: 0.4048Epoch 2/3\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 1.6550 - accuracy: 0.5700\n",
      "1523/6000 [======>.......................] - ETA: 19s - loss: 2.0143 - accuracy: 0.4377Epoch 2/3\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 1.7038 - accuracy: 0.5812\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8530 - accuracy: 0.7931\n",
      "2998/6000 [=============>................] - ETA: 12s - loss: 1.7193 - accuracy: 0.5185Epoch 3/3\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8708 - accuracy: 0.7874\n",
      "3011/6000 [==============>...............] - ETA: 12s - loss: 1.7165 - accuracy: 0.5197Epoch 3/3\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8664 - accuracy: 0.7912\n",
      "  45/1500 [..............................] - ETA: 5s - loss: 0.7104 - accuracy: 0.8208Epoch 3/3\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.9070 - accuracy: 0.7836\n",
      "3043/6000 [==============>...............] - ETA: 12s - loss: 1.7285 - accuracy: 0.5215Epoch 3/3\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8566 - accuracy: 0.8040\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.8369 - accuracy: 0.7935\n",
      "3074/6000 [==============>...............] - ETA: 12s - loss: 1.7047 - accuracy: 0.5240Epoch 3/3\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.6042 - accuracy: 0.8499\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.6203 - accuracy: 0.8439\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.6081 - accuracy: 0.8465\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.6354 - accuracy: 0.8391\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.5977 - accuracy: 0.8453\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.5995 - accuracy: 0.8541\n",
      "Epoch 4/5\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.5594 - accuracy: 0.8565\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.5526 - accuracy: 0.8555\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.5390 - accuracy: 0.8647\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.5812 - accuracy: 0.8459\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.5040 - accuracy: 0.8719\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 1.3651 - accuracy: 0.6543\n",
      "Epoch 2/5\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 1.2708 - accuracy: 0.6963\n",
      "5951/6000 [============================>.] - ETA: 0s - loss: 1.1754 - accuracy: 0.6999Epoch 2/5\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.1713 - accuracy: 0.7007\n",
      "1330/1500 [=========================>....] - ETA: 0s - loss: 0.4940 - accuracy: 0.8732Epoch 2/5\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.3417 - accuracy: 0.6557\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.2650 - accuracy: 0.6536\n",
      "  87/6000 [..............................] - ETA: 17s - loss: 0.7231 - accuracy: 0.8204Epoch 2/3\n",
      "5999/6000 [============================>.] - ETA: 0s - loss: 1.3343 - accuracy: 0.6392Epoch 2/3\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.3342 - accuracy: 0.6392\n",
      "5981/6000 [============================>.] - ETA: 0s - loss: 1.3115 - accuracy: 0.6476Epoch 2/3\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.2668 - accuracy: 0.6933\n",
      "5960/6000 [============================>.] - ETA: 0s - loss: 1.2496 - accuracy: 0.6699Epoch 2/5\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.3095 - accuracy: 0.6482\n",
      "  17/6000 [..............................] - ETA: 19s - loss: 0.8072 - accuracy: 0.8309Epoch 2/5\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.2461 - accuracy: 0.6709\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 1.3279 - accuracy: 0.6766\n",
      " 173/6000 [..............................] - ETA: 17s - loss: 0.7666 - accuracy: 0.8150Epoch 2/3\n",
      "   1/6000 [..............................] - ETA: 15s - loss: 0.7672 - accuracy: 0.7500Epoch 2/3\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.4903 - accuracy: 0.8739\n",
      " 273/6000 [>.............................] - ETA: 17s - loss: 0.7594 - accuracy: 0.8123Epoch 5/5\n",
      " 265/6000 [>.............................] - ETA: 16s - loss: 0.6424 - accuracy: 0.8354Epoch 1/5\n",
      " 264/6000 [>.............................] - ETA: 17s - loss: 0.7712 - accuracy: 0.7978Epoch 1/5\n",
      " 331/6000 [>.............................] - ETA: 16s - loss: 0.7559 - accuracy: 0.8085Epoch 1/5\n",
      "  96/1500 [>.............................] - ETA: 4s - loss: 2.2944 - accuracy: 0.247787Epoch 1/5\n",
      " 389/6000 [>.............................] - ETA: 17s - loss: 0.7165 - accuracy: 0.8210Epoch 1/3\n",
      "750/750 [==============================] - 4s 4ms/step - loss: 1.9975 - accuracy: 0.4594\n",
      "1311/6000 [=====>........................] - ETA: 17s - loss: 0.7316 - accuracy: 0.8146Epoch 2/3\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.4321 - accuracy: 0.8855\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 1.7663 - accuracy: 0.5693\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 8s 4ms/step - loss: 1.7699 - accuracy: 0.5729\n",
      "1907/6000 [========>.....................] - ETA: 15s - loss: 0.6551 - accuracy: 0.8344Epoch 2/5\n",
      "1500/1500 [==============================] - 8s 4ms/step - loss: 1.7118 - accuracy: 0.5624\n",
      "  92/1500 [>.............................] - ETA: 4s - loss: 1.1698 - accuracy: 0.7167Epoch 2/5\n",
      "1500/1500 [==============================] - 8s 4ms/step - loss: 1.6394 - accuracy: 0.5724\n",
      "1874/6000 [========>.....................] - ETA: 15s - loss: 0.6957 - accuracy: 0.8252Epoch 2/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 1.2117 - accuracy: 0.7306\n",
      "1914/6000 [========>.....................] - ETA: 16s - loss: 0.6722 - accuracy: 0.8192Epoch 3/3\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.8322 - accuracy: 0.8087\n",
      "188/188 [==============================] - 2s 5ms/step - loss: 0.7119 - accuracy: 0.8313\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.9222 - accuracy: 0.7841\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.9562 - accuracy: 0.7729\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.8662 - accuracy: 0.7820\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.8271 - accuracy: 0.7966\n",
      "Epoch 3/5\n",
      "4293/6000 [====================>.........] - ETA: 6s - loss: 0.6194 - accuracy: 0.8397Epoch 1/3\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.6442 - accuracy: 0.8422\n",
      "1384/1500 [==========================>...] - ETA: 0s - loss: 0.5963 - accuracy: 0.8487Epoch 4/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.6732 - accuracy: 0.8309\n",
      "1461/1500 [============================>.] - ETA: 0s - loss: 0.6194 - accuracy: 0.8408Epoch 4/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.6173 - accuracy: 0.8413\n",
      "1481/1500 [============================>.] - ETA: 0s - loss: 0.5936 - accuracy: 0.8491Epoch 4/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.5922 - accuracy: 0.8495\n",
      "4911/6000 [=======================>......] - ETA: 4s - loss: 0.6077 - accuracy: 0.8426Epoch 4/5\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4071 - accuracy: 0.8917\n",
      "750/750 [==============================] - 5s 4ms/step - loss: 1.9360 - accuracy: 0.4766\n",
      "5179/6000 [========================>.....] - ETA: 3s - loss: 0.5330 - accuracy: 0.8610Epoch 2/3\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 0.6245 - accuracy: 0.8377\n",
      "Epoch 3/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 1.1233 - accuracy: 0.7446\n",
      "5869/6000 [============================>.] - ETA: 0s - loss: 0.6315 - accuracy: 0.8330Epoch 3/3\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 0.5777 - accuracy: 0.8493\n",
      "1080/1500 [====================>.........] - ETA: 1s - loss: 0.5306 - accuracy: 0.8646Epoch 3/5\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 0.5221 - accuracy: 0.8630\n",
      "Epoch 3/5\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 0.5834 - accuracy: 0.8428\n",
      "1154/1500 [======================>.......] - ETA: 1s - loss: 0.5287 - accuracy: 0.8649Epoch 3/5\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 0.6057 - accuracy: 0.8446\n",
      " 294/6000 [>.............................] - ETA: 21s - loss: 0.5469 - accuracy: 0.8474Epoch 3/3\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 0.6286 - accuracy: 0.8339\n",
      "1093/1500 [====================>.........] - ETA: 1s - loss: 0.5162 - accuracy: 0.8628Epoch 3/3\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 0.5901 - accuracy: 0.8459\n",
      "  12/6000 [..............................] - ETA: 28s - loss: 0.5176 - accuracy: 0.8542Epoch 3/5\n",
      "6000/6000 [==============================] - 24s 4ms/step - loss: 0.5908 - accuracy: 0.8393\n",
      "5937/6000 [============================>.] - ETA: 0s - loss: 0.5791 - accuracy: 0.8473Epoch 3/3\n",
      "6000/6000 [==============================] - 25s 4ms/step - loss: 0.6011 - accuracy: 0.8445\n",
      "1275/1500 [========================>.....] - ETA: 0s - loss: 0.5239 - accuracy: 0.8663Epoch 3/3\n",
      "6000/6000 [==============================] - 25s 4ms/step - loss: 0.5782 - accuracy: 0.8475\n",
      "1161/1500 [======================>.......] - ETA: 1s - loss: 0.4924 - accuracy: 0.8712Epoch 3/3\n",
      "342/750 [============>.................] - ETA: 1s - loss: 0.8383 - accuracy: 0.8037Epoch 1/3\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.5212 - accuracy: 0.8668\n",
      " 341/6000 [>.............................] - ETA: 23s - loss: 0.4953 - accuracy: 0.8611Epoch 5/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.5434 - accuracy: 0.8585\n",
      " 270/6000 [>.............................] - ETA: 24s - loss: 0.4600 - accuracy: 0.8810Epoch 5/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.5063 - accuracy: 0.8660\n",
      " 475/6000 [=>............................] - ETA: 22s - loss: 0.4943 - accuracy: 0.8637Epoch 5/5\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.4848 - accuracy: 0.8724\n",
      " 450/6000 [=>............................] - ETA: 22s - loss: 0.5162 - accuracy: 0.8622Epoch 5/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.7725 - accuracy: 0.8167\n",
      "750/750 [==============================] - 5s 4ms/step - loss: 1.9670 - accuracy: 0.4956\n",
      " 888/6000 [===>..........................] - ETA: 21s - loss: 0.4908 - accuracy: 0.8692Epoch 2/3\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.6929 - accuracy: 0.8193\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 1.1697 - accuracy: 0.7210\n",
      "1267/1500 [========================>.....] - ETA: 0s - loss: 0.4292 - accuracy: 0.8862Epoch 3/3\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.4551 - accuracy: 0.8791\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.4720 - accuracy: 0.8732\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.4446 - accuracy: 0.8798\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.4265 - accuracy: 0.8859\n",
      "2257/6000 [==========>...................] - ETA: 14s - loss: 0.4962 - accuracy: 0.8669Epoch 1/3\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4665 - accuracy: 0.8733\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.8096 - accuracy: 0.8081\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.4389 - accuracy: 0.8794\n",
      "375/375 [==============================] - 2s 3ms/step - loss: 0.3810 - accuracy: 0.8969\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.7304 - accuracy: 0.8180\n",
      "750/750 [==============================] - 4s 4ms/step - loss: 1.9689 - accuracy: 0.4497\n",
      "Epoch 2/3\n",
      "3503/6000 [================>.............] - ETA: 9s - loss: 0.4866 - accuracy: 0.8695Epoch 1/3\n",
      "3741/6000 [=================>............] - ETA: 8s - loss: 0.4550 - accuracy: 0.8785Epoch 1/5\n",
      "3594/6000 [================>.............] - ETA: 9s - loss: 0.4859 - accuracy: 0.8696Epoch 1/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 1.1880 - accuracy: 0.7211\n",
      "3895/6000 [==================>...........] - ETA: 7s - loss: 0.4580 - accuracy: 0.8740Epoch 3/3\n",
      "4073/6000 [===================>..........] - ETA: 7s - loss: 0.4790 - accuracy: 0.8713Epoch 1/5\n",
      "750/750 [==============================] - 5s 4ms/step - loss: 2.0151 - accuracy: 0.4496\n",
      "4269/6000 [====================>.........] - ETA: 6s - loss: 0.4798 - accuracy: 0.8708Epoch 2/3\n",
      "750/750 [==============================] - 5s 4ms/step - loss: 1.9850 - accuracy: 0.5581\n",
      "4214/6000 [====================>.........] - ETA: 6s - loss: 0.4647 - accuracy: 0.8766Epoch 2/5\n",
      "750/750 [==============================] - 5s 4ms/step - loss: 2.0052 - accuracy: 0.4163\n",
      " 73/750 [=>............................] - ETA: 2s - loss: 1.5571 - accuracy: 0.6244Epoch 2/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.8203 - accuracy: 0.8035\n",
      "750/750 [==============================] - 5s 4ms/step - loss: 2.0092 - accuracy: 0.4454\n",
      "5269/6000 [=========================>....] - ETA: 2s - loss: 0.4792 - accuracy: 0.8715Epoch 2/5\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.7241 - accuracy: 0.8193\n",
      "375/375 [==============================] - 2s 4ms/step - loss: 0.4492 - accuracy: 0.8779\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 1.2631 - accuracy: 0.7030\n",
      "673/750 [=========================>....] - ETA: 0s - loss: 1.2476 - accuracy: 0.7027Epoch 3/3\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 1.2245 - accuracy: 0.7071\n",
      "5290/6000 [=========================>....] - ETA: 2s - loss: 0.4460 - accuracy: 0.8794Epoch 3/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 1.2521 - accuracy: 0.6763\n",
      "Epoch 3/5\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.4760 - accuracy: 0.8717\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 1.2826 - accuracy: 0.6748\n",
      "5601/6000 [===========================>..] - ETA: 1s - loss: 0.4603 - accuracy: 0.8731Epoch 3/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.8856 - accuracy: 0.7912\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.4414 - accuracy: 0.8804\n",
      "5794/6000 [===========================>..] - ETA: 0s - loss: 0.4607 - accuracy: 0.8730Epoch 4/5\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.4062 - accuracy: 0.8890\n",
      "5778/6000 [===========================>..] - ETA: 0s - loss: 0.4429 - accuracy: 0.8811Epoch 4/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.8699 - accuracy: 0.7830\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.8929 - accuracy: 0.7780\n",
      "5889/6000 [============================>.] - ETA: 0s - loss: 0.4609 - accuracy: 0.8730Epoch 4/5\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.4493 - accuracy: 0.8766\n",
      "5957/6000 [============================>.] - ETA: 0s - loss: 0.4464 - accuracy: 0.8816Epoch 4/5\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.4465 - accuracy: 0.8816\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.4460 - accuracy: 0.8800\n",
      "5909/6000 [============================>.] - ETA: 0s - loss: 0.4417 - accuracy: 0.8812Epoch 4/5\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.4705 - accuracy: 0.8725\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.4600 - accuracy: 0.8733\n",
      " 264/6000 [>.............................] - ETA: 19s - loss: 0.4056 - accuracy: 0.8859Epoch 1/5\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.4416 - accuracy: 0.8811\n",
      "222/750 [=======>......................] - ETA: 1s - loss: 0.7629 - accuracy: 0.8098Epoch 1/5\n",
      "6000/6000 [==============================] - 23s 4ms/step - loss: 0.4566 - accuracy: 0.8772\n",
      "188/188 [==============================] - 1s 4ms/step - loss: 0.7492 - accuracy: 0.8312\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.9118 - accuracy: 0.7667\n",
      "262/750 [=========>....................] - ETA: 1s - loss: 2.2430 - accuracy: 0.3175Epoch 4/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.6949 - accuracy: 0.8206\n",
      " 370/1500 [======>.......................] - ETA: 3s - loss: 0.4563 - accuracy: 0.8760Epoch 5/5\n",
      "750/750 [==============================] - 3s 3ms/step - loss: 0.7126 - accuracy: 0.8205\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 4s 3ms/step - loss: 2.0314 - accuracy: 0.4840\n",
      "159/750 [=====>........................] - ETA: 2s - loss: 0.6416 - accuracy: 0.8397Epoch 2/5\n",
      "750/750 [==============================] - 4s 3ms/step - loss: 1.9692 - accuracy: 0.4797\n",
      " 645/1500 [===========>..................] - ETA: 2s - loss: 0.4484 - accuracy: 0.8733Epoch 2/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.7183 - accuracy: 0.8199\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.5899 - accuracy: 0.8443\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 0.6025 - accuracy: 0.8448\n",
      "750/750 [==============================] - 3s 3ms/step - loss: 1.2867 - accuracy: 0.7002\n",
      "1301/1500 [=========================>....] - ETA: 0s - loss: 0.4293 - accuracy: 0.8801Epoch 3/5\n",
      "1500/1500 [==============================] - 6s 3ms/step - loss: 0.4249 - accuracy: 0.8828\n",
      "750/750 [==============================] - 3s 4ms/step - loss: 1.1847 - accuracy: 0.7210\n",
      " 23/188 [==>...........................] - ETA: 0s - loss: 0.5980 - accuracy: 0.8295  Epoch 3/5\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.4014 - accuracy: 0.8888\n",
      "1500/1500 [==============================] - 6s 3ms/step - loss: 0.4529 - accuracy: 0.8742\n",
      "1500/1500 [==============================] - 6s 3ms/step - loss: 0.4246 - accuracy: 0.8813\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.5712 - accuracy: 0.8402\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.4154 - accuracy: 0.8881\n",
      "188/188 [==============================] - 1s 3ms/step - loss: 0.5550 - accuracy: 0.8587\n",
      "750/750 [==============================] - 3s 3ms/step - loss: 0.5998 - accuracy: 0.8496\n",
      "188/188 [==============================] - 1s 2ms/step - loss: 0.5794 - accuracy: 0.8467\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.9161 - accuracy: 0.7784\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 2s 3ms/step - loss: 0.8373 - accuracy: 0.7949\n",
      "Epoch 4/5\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.7328 - accuracy: 0.8196\n",
      "Epoch 5/5\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6652 - accuracy: 0.8332\n",
      "3819/6000 [==================>...........] - ETA: 6s - loss: 0.4259 - accuracy: 0.8846Epoch 5/5\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.6206 - accuracy: 0.8448\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.5611 - accuracy: 0.8575\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.5518 - accuracy: 0.8665\n",
      "188/188 [==============================] - 0s 1ms/step - loss: 0.5363 - accuracy: 0.8612\n",
      "6000/6000 [==============================] - 14s 2ms/step - loss: 0.4140 - accuracy: 0.8872\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 13s 2ms/step - loss: 0.3861 - accuracy: 0.8928\n",
      " 280/6000 [>.............................] - ETA: 6s - loss: 0.3547 - accuracy: 0.8960Epoch 5/5\n",
      "6000/6000 [==============================] - 13s 2ms/step - loss: 0.3919 - accuracy: 0.8914\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 13s 2ms/step - loss: 0.3585 - accuracy: 0.8989\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 13s 2ms/step - loss: 0.3861 - accuracy: 0.8935\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 7s 1ms/step - loss: 0.3799 - accuracy: 0.8945\n",
      "6000/6000 [==============================] - 8s 1ms/step - loss: 0.3562 - accuracy: 0.9000\n",
      "6000/6000 [==============================] - 7s 1ms/step - loss: 0.3596 - accuracy: 0.8992\n",
      "6000/6000 [==============================] - 8s 1ms/step - loss: 0.3311 - accuracy: 0.9057\n",
      "6000/6000 [==============================] - 8s 1ms/step - loss: 0.3529 - accuracy: 0.9003\n",
      "1500/1500 [==============================] - 1s 819us/step - loss: 0.3455 - accuracy: 0.9033\n",
      "1500/1500 [==============================] - 1s 766us/step - loss: 0.3405 - accuracy: 0.9067\n",
      "1500/1500 [==============================] - 1s 769us/step - loss: 0.3691 - accuracy: 0.8945\n",
      "1500/1500 [==============================] - 1s 755us/step - loss: 0.3463 - accuracy: 0.9001\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 0.3634 - accuracy: 0.8974\n",
      "Epoch 1/5\n",
      "7500/7500 [==============================] - 6s 778us/step - loss: 1.2169 - accuracy: 0.6776\n",
      "Epoch 2/5\n",
      "7500/7500 [==============================] - 5s 692us/step - loss: 0.5618 - accuracy: 0.8515\n",
      "Epoch 3/5\n",
      "7500/7500 [==============================] - 5s 666us/step - loss: 0.4275 - accuracy: 0.8823\n",
      "Epoch 4/5\n",
      "7500/7500 [==============================] - 5s 669us/step - loss: 0.3716 - accuracy: 0.8961\n",
      "Epoch 5/5\n",
      "7500/7500 [==============================] - 5s 671us/step - loss: 0.3406 - accuracy: 0.9035\n",
      "Best: 0.9004000067710877 using {'batch_size': 8, 'epochs': 5, 'units': 32}\n",
      "Means: 0.8830333471298217, Stdev: 0.005286411164782275 with: {'batch_size': 8, 'epochs': 3, 'units': 32}\n",
      "Means: 0.9004000067710877, Stdev: 0.004282200176118761 with: {'batch_size': 8, 'epochs': 5, 'units': 32}\n",
      "Means: 0.858899998664856, Stdev: 0.008815596651594286 with: {'batch_size': 32, 'epochs': 3, 'units': 32}\n",
      "Means: 0.8838500022888184, Stdev: 0.008948369435317209 with: {'batch_size': 32, 'epochs': 5, 'units': 32}\n",
      "Means: 0.8238333344459534, Stdev: 0.006075459194424741 with: {'batch_size': 64, 'epochs': 3, 'units': 32}\n",
      "Means: 0.8546500086784363, Stdev: 0.009728300675470715 with: {'batch_size': 64, 'epochs': 5, 'units': 32}\n"
     ]
    }
   ],
   "source": [
    "# func to create model, required for KerasClassifier\n",
    "def create_model(units=32):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # 1 hidden layer\n",
    "    model.add(Dense(\n",
    "        units, # number of neurons in our hidden layer\n",
    "        input_dim=784, # implicitly declaring our input layer\n",
    "        activation='relu'\n",
    "    ))\n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax')) # 10 unit/ neurons in output layer because we have 10 possible labels to predict, use softmax for a label set greater than 2\n",
    "    # compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # do not include model.fit() inside the create_model func, kerasclassifier is expecting a compiled model\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# dfine the grid search params\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search params\n",
    "param_grid = {'batch_size': [8, 32,64], 'epochs': [3, 5], 'units': [32]}\n",
    "\n",
    "# create grid search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, verbose=1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# report results\n",
    "print(f'Best: {grid_result.best_score_} using {grid_result.best_params_}')\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f'Means: {mean}, Stdev: {stdev} with: {param}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e860b3d-3769-4fb9-9d4c-92d0ff7f1cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a8e44e5-c893-41b8-ae69-3a3121d1a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bf92887-274e-4cee-acac-2c37d100dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32]))\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.RealInterval(0.001, .01))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_NUM_UNITS, HP_LEARNING_RATE, HP_OPTIMIZER],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8a6e75a-d751-4f44-88ef-7813ff84595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(hparams):\n",
    "    # hyperparams\n",
    "    # Sequential() model\n",
    "    # hparmas is a standard python dict with keys and vals\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(hparams[HP_NUM_UNITS], activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    # get optimizer from param dict\n",
    "    opt_name = hparams[HP_OPTIMIZER]\n",
    "    # get learning_rate for optimizer\n",
    "    lr = hparams[HP_LEARNING_RATE]\n",
    "    \n",
    "    if opt_name == 'adam':\n",
    "        # import Adam opt object and set learning rate\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        # import sgd opt object and set learning rate\n",
    "    elif opt_name == 'sgd':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "    else:\n",
    "        raise ValueError(f'unexpected optimizer name: {opt_name}')\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=5)\n",
    "    _, accuracy = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de996b31-9890-4050-821b-33711526a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        # record the values used in this trial\n",
    "        hp.hparams(hparams)\n",
    "        # call train_test_model to build, train, and score model on parameter values\n",
    "        accuracy = train_test_model(hparams)\n",
    "        # store trained accuracy to file\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "672e8aa1-ebe0-4d8a-87cb-614963d8f08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'num_units': 16, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 724us/step - loss: 1.6955 - accuracy: 0.5797\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 748us/step - loss: 0.8676 - accuracy: 0.7803\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 753us/step - loss: 0.6299 - accuracy: 0.8350\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 755us/step - loss: 0.5194 - accuracy: 0.8615\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 772us/step - loss: 0.4549 - accuracy: 0.8777\n",
      "313/313 [==============================] - 0s 593us/step - loss: 0.4216 - accuracy: 0.8874\n",
      "--- Starting trial: run-1\n",
      "{'num_units': 16, 'learning_rate': 0.001, 'optimizer': 'sgd'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 633us/step - loss: 2.3020 - accuracy: 0.1100\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 604us/step - loss: 2.3015 - accuracy: 0.1124\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 623us/step - loss: 2.3012 - accuracy: 0.1124\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 623us/step - loss: 2.3010 - accuracy: 0.1124\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 636us/step - loss: 2.3009 - accuracy: 0.1124\n",
      "313/313 [==============================] - 0s 531us/step - loss: 2.3007 - accuracy: 0.1135\n",
      "--- Starting trial: run-2\n",
      "{'num_units': 16, 'learning_rate': 0.01, 'optimizer': 'adam'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 726us/step - loss: 0.7290 - accuracy: 0.7951\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 739us/step - loss: 0.3706 - accuracy: 0.8928\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 718us/step - loss: 0.3331 - accuracy: 0.9031\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 723us/step - loss: 0.3109 - accuracy: 0.9108\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 718us/step - loss: 0.2983 - accuracy: 0.9155\n",
      "313/313 [==============================] - 0s 533us/step - loss: 0.2939 - accuracy: 0.9148\n",
      "--- Starting trial: run-3\n",
      "{'num_units': 16, 'learning_rate': 0.01, 'optimizer': 'sgd'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 594us/step - loss: 2.3012 - accuracy: 0.1126\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 627us/step - loss: 2.3006 - accuracy: 0.1124\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 633us/step - loss: 2.3004 - accuracy: 0.1124\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 624us/step - loss: 2.3001 - accuracy: 0.1124\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 629us/step - loss: 2.2999 - accuracy: 0.1124\n",
      "313/313 [==============================] - 0s 609us/step - loss: 2.2995 - accuracy: 0.1135\n",
      "--- Starting trial: run-4\n",
      "{'num_units': 32, 'learning_rate': 0.001, 'optimizer': 'adam'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 742us/step - loss: 1.5641 - accuracy: 0.6277\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 767us/step - loss: 0.7160 - accuracy: 0.8321\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 740us/step - loss: 0.5088 - accuracy: 0.8710\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 727us/step - loss: 0.4268 - accuracy: 0.8861\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 736us/step - loss: 0.3843 - accuracy: 0.8951\n",
      "313/313 [==============================] - 0s 551us/step - loss: 0.3575 - accuracy: 0.9020\n",
      "--- Starting trial: run-5\n",
      "{'num_units': 32, 'learning_rate': 0.001, 'optimizer': 'sgd'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 640us/step - loss: 2.3019 - accuracy: 0.1207\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 645us/step - loss: 2.3014 - accuracy: 0.1124\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 635us/step - loss: 2.3011 - accuracy: 0.1124\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 624us/step - loss: 2.3009 - accuracy: 0.1124\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 627us/step - loss: 2.3008 - accuracy: 0.1124\n",
      "313/313 [==============================] - 0s 550us/step - loss: 2.3006 - accuracy: 0.1135\n",
      "--- Starting trial: run-6\n",
      "{'num_units': 32, 'learning_rate': 0.01, 'optimizer': 'adam'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 2s 755us/step - loss: 0.6880 - accuracy: 0.8026\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 727us/step - loss: 0.3329 - accuracy: 0.9043\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 737us/step - loss: 0.2966 - accuracy: 0.9137\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 740us/step - loss: 0.2804 - accuracy: 0.9190\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 753us/step - loss: 0.2680 - accuracy: 0.9224\n",
      "313/313 [==============================] - 0s 616us/step - loss: 0.2559 - accuracy: 0.9263\n",
      "--- Starting trial: run-7\n",
      "{'num_units': 32, 'learning_rate': 0.01, 'optimizer': 'sgd'}\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 1s 650us/step - loss: 2.3008 - accuracy: 0.1150\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 1s 643us/step - loss: 2.3003 - accuracy: 0.1124\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 1s 634us/step - loss: 2.3001 - accuracy: 0.1149\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 1s 640us/step - loss: 2.2998 - accuracy: 0.1124\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 1s 660us/step - loss: 2.2995 - accuracy: 0.1124\n",
      "313/313 [==============================] - 0s 569us/step - loss: 2.2990 - accuracy: 0.1135\n"
     ]
    }
   ],
   "source": [
    "# this is the main method, starting reading code from here\n",
    "session_num = 0\n",
    "\n",
    "for num_units in HP_NUM_UNITS.domain.values:\n",
    "    for learning_rate in (HP_LEARNING_RATE.domain.min_value, HP_LEARNING_RATE.domain.max_value):\n",
    "        for optimizer in HP_OPTIMIZER.domain.values:\n",
    "            # as we loop through all hyper-param values, store each unique combination in the dictionary hparams\n",
    "            hparams = {\n",
    "                HP_NUM_UNITS: num_units,\n",
    "                HP_LEARNING_RATE: learning_rate,\n",
    "                HP_OPTIMIZER: optimizer\n",
    "            }\n",
    "            \n",
    "            run_name = f'run-{session_num}'\n",
    "            print(f'--- Starting trial: {run_name}')\n",
    "            print({h.name: hparams[h] for h in hparams})\n",
    "            # execute the run func, which runs the training of the models\n",
    "            run('logs/hparam_tuning/' + run_name, hparams)\n",
    "            session_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbbb7892-e3c1-4242-99e3-1eeb5c0c3793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-96f6c87a680da01\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-96f6c87a680da01\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ffbdafa-39ae-4c42-96c3-f40e4340934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras-tuner --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c11769b0-b4ee-42ca-927f-c648affa6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras_tuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7903080c-7e28-4b82-b479-9e2b76d2ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This model Tunes:\n",
    "- Number of Neurons in the Hidden Layer\n",
    "- Learning Rate in Adam\n",
    "'''\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=32,\n",
    "                                        max_value=512,\n",
    "                                        step=32),\n",
    "                           activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b29f66a3-dc2f-4482-9be3-83b13f547474",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='./keras-tuner-trial',\n",
    "    project_name='mnist'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b9de72f-ffb5-4ff6-b3b3-7ace116033c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 2\n",
      "units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 32, 'sampling': None}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a387440c-c310-40bd-8c86-0b8d5c42d493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Complete [00h 00m 30s]\n",
      "val_accuracy: 0.8305000066757202\n",
      "\n",
      "Best val_accuracy So Far: 0.9129666487375895\n",
      "Total elapsed time: 00h 02m 57s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X_train, y_train, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c972674c-fd32-44fa-9ab5-4344bc413080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in ./keras-tuner-trial/mnist\n",
      "Showing 10 best trials\n",
      "Objective(name='val_accuracy', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 128\n",
      "learning_rate: 0.001\n",
      "Score: 0.9129666487375895\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 320\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8543333411216736\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 288\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8509999910990397\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 192\n",
      "learning_rate: 0.0001\n",
      "Score: 0.8305000066757202\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units: 96\n",
      "learning_rate: 0.0001\n",
      "Score: 0.7930333216985067\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a223c37-087c-4ca7-a62c-f682893677c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
